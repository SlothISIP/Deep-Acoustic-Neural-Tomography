% ICASSP 2026 Submission
% Neural Acoustic Diffraction Tomography
% ========================================================================
\documentclass[conference]{IEEEtran}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}

\graphicspath{{../results/paper_figures/}}

% Math shortcuts
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\Lsdf}{\mathcal{L}_{\text{sdf}}}
\newcommand{\Leik}{\mathcal{L}_{\text{eik}}}
\newcommand{\Lcyc}{\mathcal{L}_{\text{cycle}}}
\newcommand{\pinc}{p_{\text{inc}}}
\newcommand{\pscat}{p_{\text{scat}}}
\newcommand{\ptot}{p_{\text{tot}}}

\begin{document}

% ========================================================================
\title{Neural Acoustic Diffraction Tomography:\\
Cycle-Consistent Geometry Reconstruction from 2D BEM Data}

\author{
\IEEEauthorblockN{Anonymous Author(s)}
\IEEEauthorblockA{Submitted for blind review}
}

\maketitle

% ========================================================================
% ABSTRACT
% ========================================================================
\begin{abstract}
We present a neural framework for 2D acoustic diffraction tomography that
reconstructs scene geometry from boundary element method (BEM) simulations.
Our approach introduces a \emph{transfer function} formulation that learns
the scattered-to-incident pressure ratio, reducing reconstruction error from
48\% (vanilla MLP on raw pressure) to 4.47\% across 15 synthetic scenes
while providing $2{,}000\times$ speedup over BEM inference.
An \emph{auto-decoder} inverse model maps acoustic observations to
signed distance functions (SDF) with Eikonal regularization, yielding
$0.91 \pm 0.01$ mean intersection-over-union (IoU), validated through
cycle-consistent forward--inverse agreement (Pearson $r=0.91$).
We further analyze why Helmholtz PDE enforcement fails for neural surrogates:
the autodiff Laplacian $\nabla^2 f_\theta$ correlates with the physical
Laplacian at only $r = 0.19$ (3.5\% variance explained), exacerbated by
random Fourier features that amplify second derivatives by
$4\pi^2\sigma^2 \approx 35{,}000\times$ at $\sigma = 30$.
\end{abstract}

\begin{IEEEkeywords}
acoustic diffraction, neural surrogate, signed distance function,
inverse scattering, cycle-consistency, boundary element method
\end{IEEEkeywords}

% ========================================================================
% 1. INTRODUCTION
% ========================================================================
\section{Introduction}
\label{sec:intro}

Reconstructing the geometry of a scene from acoustic measurements is a
fundamental problem in computational acoustics with applications to room
modeling~\cite{kuster2004acoustic}, sonar imaging~\cite{colton2019inverse},
and augmented reality~\cite{richard2022deep}.
Classical approaches rely on iterative optimization against physics-based
solvers~\cite{colton2019inverse}, which is computationally expensive and
sensitive to initialization.
Recent neural approaches learn implicit representations of acoustic
fields~\cite{luo2022naf, su2023inras} or jointly model audio and
geometry~\cite{liang2024acoustic}, but model the \emph{total}
pressure without recovering scene geometry.
Acoustic NLOS imaging~\cite{lindell2019acoustic} reconstructs hidden
objects via time-of-flight backprojection with known relay surfaces,
and EchoScan~\cite{yeon2024echoscan} infers enclosed room geometry from
microphone-array RIRs.
Vla\v{s}i\'c et al.~\cite{vlasic2022implicit} represent obstacles as SDF
zero-level-sets for inverse scattering but rely on a classical boundary
integral solver at each iteration.
Our work replaces the PDE solver with a learned transfer function
surrogate ($2{,}000\times$ speedup) and adds cycle-consistency through
the neural forward model for geometry validation.
Physics-informed neural networks~\cite{raissi2019physics} offer PDE
supervision, but as we show, Helmholtz enforcement fails when applied
through neural surrogates with high-bandwidth Fourier
features~\cite{wang2021eigenvector}.

We propose a two-stage neural framework for 2D acoustic diffraction
tomography (Fig.~\ref{fig:architecture}). In the \emph{forward} stage,
we learn a transfer function $T = \pscat / \pinc$ that captures only the
scattering component, removing the dominant free-space phase oscillation.
This reformulation substantially reduces target complexity: across our
dataset, a per-scene constant prediction of $T$ explains 89.6\% of
variance (vs.\ 13\% for raw $\pscat$), as the transfer function removes
the dominant phase oscillation spanning 12 cycles over 2--8\,kHz.
This enables a compact MLP to approximate BEM-quality fields.
In the \emph{inverse} stage, an auto-decoder~\cite{park2019deepsdf}
optimizes per-scene latent codes into an SDF decoder with Eikonal
regularization, and a frozen copy of the forward model provides
cycle-consistency supervision.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{fig_1_architecture.pdf}
\caption{System architecture. The forward model learns a transfer function
$T = \pscat/\pinc$; the inverse model maps acoustic data to SDF via an
auto-decoder. Cycle-consistency closes the loop. Note: Helmholtz loss
$\mathcal{L}_{\text{helm}}$ was found to be incompatible with neural
surrogates and is \emph{disabled} (Sec.~\ref{sec:discussion}).}
\label{fig:architecture}
\end{figure}

Our contributions are:
\begin{enumerate}
\item A \textbf{transfer function formulation} $T = \pscat/\pinc$ that
      reduces forward error from 48\% to 4.47\% at $2{,}000\times$ BEM
      speedup on 15 scenes with 200 frequencies (Sec.~\ref{sec:forward}).
\item An \textbf{auto-decoder inverse model} with Eikonal-regularized SDF
      that reconstructs geometry at $0.91 \pm 0.01$ mean IoU, validated
      via cycle-consistency ($r = 0.91$) and robust to 10\,dB noise
      (Sec.~\ref{sec:inverse}).
\item \textbf{Analysis of Helmholtz PDE failure} in neural surrogates:
      we show that the autodiff Laplacian explains only 3.5\% of the
      physical Laplacian's variance ($r = 0.19$), caused by
      $\mathcal{O}(10^4)$ second-derivative amplification from random
      Fourier features (Sec.~\ref{sec:discussion}).
\end{enumerate}
We further evaluate noise robustness, seed reproducibility, leave-one-out
generalization, and cross-frequency transfer (Sec.~\ref{sec:robustness}).

% ========================================================================
% 2. METHOD
% ========================================================================
\section{Method}
\label{sec:method}

% -----------------------------------------------------------------------
\subsection{Problem Formulation}
\label{sec:problem}

Consider a 2D acoustic scene with scatterers occupying region $\Omega$
bounded by surface $\partial\Omega$. A point source at position
$\bm{x}_s$ emits a monochromatic wave at wavenumber
$k = 2\pi f / c$, where $c = 343$\,m/s.
The total pressure $\ptot(\bm{x})$ satisfies the Helmholtz equation
$(\nabla^2 + k^2)\ptot = -\delta(\bm{x} - \bm{x}_s)$ in the exterior
domain, with Neumann boundary conditions on $\partial\Omega$.
The scattered field is $\pscat = \ptot - \pinc$, where
$\pinc = \frac{i}{4}H_0^{(1)}(k|\bm{x} - \bm{x}_s|)$ is the free-space
Green's function in 2D. Edge diffraction~\cite{kouyoumjian1974uniform}
produces the dominant contribution to $\pscat$ in our scenes.

We seek to learn: (i)~a forward surrogate
$f_\theta: (\bm{x}_s, \bm{x}_r, k) \mapsto \ptot$ that approximates BEM,
and (ii)~an inverse mapping
$g_\phi: \{\ptot\} \mapsto s(\bm{x})$ that recovers the signed distance
function $s: \RR^2 \to \RR$ of $\partial\Omega$.

% -----------------------------------------------------------------------
\subsection{Forward Model: Transfer Function Learning}
\label{sec:forward}

\textbf{Transfer function target.}
Rather than learning $\ptot$ directly, we define the transfer function
\begin{equation}
T(\bm{x}_s, \bm{x}_r, k) = \frac{\pscat(\bm{x}_s, \bm{x}_r, k)}
    {\pinc(\bm{x}_s, \bm{x}_r, k)},
\label{eq:transfer}
\end{equation}
which removes the dominant $H_0^{(1)}(kr) \sim e^{ikr}/\sqrt{r}$ oscillation
from the learning target. This is analogous to learning a scattering matrix rather than the
total field. The total pressure is recovered as
$\ptot = \pinc \cdot (1 + T \cdot \sigma)$,
where $\sigma$ is a per-scene normalization scale.

\textbf{Architecture.}
The forward model $f_\theta$ takes 9 scalar inputs: source and receiver
coordinates $(\bm{x}_s, \bm{x}_r) \in \RR^4$, wavenumber $k$,
source--receiver distance $d$, signed distance at the receiver
$s(\bm{x}_r)$, and spatial derivatives $({\partial s}/{\partial x},
{\partial s}/{\partial y})$. These are encoded via random Fourier
features~\cite{tancik2020fourier}
($D=128$, bandwidth $\sigma_{\text{FF}} = 30$\,m$^{-1}$), concatenated
with a learnable scene embedding $e_s \in \RR^{32}$.
The network consists of 8 residual blocks with hidden dimension 768, outputting
$(\text{Re}(T), \text{Im}(T)) \in \RR^2$.
The SDF features $(s, \partial s/\partial x, \partial s/\partial y)$ condition
the forward model on geometry; during inverse training
(Sec.~\ref{sec:cycle}), these carry gradients from the SDF decoder.

\textbf{Ensemble and calibration.}
We train four models with different seeds and apply a linear calibration layer
$T_{\text{calib}} = aT_{\text{pred}} + b$ on a held-out validation set.
The ensemble reduces per-model variance and achieves 4.47\% overall error
(Sec.~\ref{sec:forward_results}).

% -----------------------------------------------------------------------
\subsection{Inverse Model: Auto-Decoder SDF Reconstruction}
\label{sec:inverse}

\textbf{SDF decoder.}
Following DeepSDF~\cite{park2019deepsdf}, we use an auto-decoder architecture
where each scene $i$ has a learnable latent code $\bm{z}_i \in \RR^{256}$.
The SDF decoder $D_\psi$ takes Fourier-encoded 2D coordinates
$\gamma(\bm{x})$ (bandwidth $\sigma = 10$) concatenated with $\bm{z}_i$
and outputs a signed distance value through 6 residual blocks (hidden
dimension 256):
\begin{equation}
s_i(\bm{x}) = D_\psi(\gamma(\bm{x}), \bm{z}_i).
\label{eq:sdf_decoder}
\end{equation}

\textbf{Multi-code composition.}
For multi-body scenes (e.g., our Scene 12 with two disjoint objects), we assign
$K$ latent codes $\{\bm{z}_i^{(k)}\}_{k=1}^K$ and compose via smooth minimum:
\begin{equation}
s_i(\bm{x}) = -\frac{1}{\alpha}\log\sum_{k=1}^K
    \exp\bigl(-\alpha \cdot D_\psi(\gamma(\bm{x}), \bm{z}_i^{(k)})\bigr),
\label{eq:smooth_min}
\end{equation}
with sharpness $\alpha = 50$, approximating $\min_k s_i^{(k)}$.

\textbf{Loss function.}
The total loss is:
\begin{equation}
\mathcal{L} = \Lsdf + \lambda_1 \Leik + \lambda_2 \Lcyc,
\label{eq:loss}
\end{equation}
where $\Lsdf = \mathbb{E}[|D_\psi(\bm{x}) - s^*(\bm{x})|]$ is the
L1 SDF supervision, $\Leik = \mathbb{E}[(|\nabla_{\bm{x}} s| - 1)^2]$ is
the Eikonal constraint enforcing $|\nabla s| = 1$, and $\Lcyc$ is the
cycle-consistency loss (Sec.~\ref{sec:cycle}). We set
$\lambda_1 = 0.1$, $\lambda_2 = 0.01$.

\textbf{Boundary oversampling.}
We oversample SDF training points near $s(\bm{x}) \approx 0$ by a factor of
$3\times$, which is critical for resolving thin geometries (ablation in
Table~\ref{tab:inverse_ablation}).

\textbf{On Helmholtz PDE loss.}
A natural extension would add a Helmholtz residual loss
$\|\nabla^2 \hat{p} + k^2 \hat{p}\|^2$ via the forward surrogate.
However, this \emph{degrades} reconstruction: the neural $\nabla^2$
captures network curvature rather than physical pressure gradients,
due to second-derivative amplification in the Fourier features.
We analyze this failure in detail in Sec.~\ref{sec:discussion}.

% -----------------------------------------------------------------------
\subsection{Cycle-Consistency}
\label{sec:cycle}

The cycle-consistency loss connects the forward and inverse models:
\begin{equation}
\Lcyc = \mathbb{E}\bigl[\|f_\theta(\bm{x}_s, \bm{x}_r, k; s_i)
    - \ptot^{\text{BEM}}\|^2\bigr],
\label{eq:cycle}
\end{equation}
where $s_i(\bm{x}_r) = D_\psi(\gamma(\bm{x}_r), \bm{z}_i)$ is evaluated
at receiver positions and fed as an additional feature to the frozen forward
model $f_\theta$. The forward model parameters are frozen during inverse
training; only $\bm{z}_i$ and $D_\psi$ are updated.

This creates a differentiable loop: latent code $\bm{z}_i \to$
SDF at receivers $\to$ forward prediction $\to$ comparison with BEM data.
The gradient flows through the SDF decoder, providing acoustic supervision
for geometry beyond the SDF loss alone.
We use the MSE loss~\eqref{eq:cycle} for training, and report Pearson
correlation $r$ as the evaluation metric (Sec.~\ref{sec:robustness})
because it is scale-invariant and interpretable.

% ========================================================================
% 3. EXPERIMENTS
% ========================================================================
\section{Experiments}
\label{sec:experiments}

% -----------------------------------------------------------------------
\subsection{Dataset}
\label{sec:dataset}

We generate 2D BEM data for 15 scenes spanning 4 geometry classes:
wedges (4), cylinders (2), polygons and barriers (6), and multi-body
compositions (3). For each scene, 3 source positions illuminate the
geometry, with receivers placed at 40--200 positions per source.
We solve the BEM~\cite{ihlenburg1998finite} at 200 frequencies uniformly
spaced in 2--8\,kHz ($k \in [36.6, 146.5]$\,rad/m), yielding 1,769,400
complex pressure observations in total. The BEM solver is validated against
Macdonald's analytical solution~\cite{macdonald1902electric} for a
90$^\circ$ wedge (1.77\% $L_2$ error).

Room impulse responses (RIRs) are synthesized via inverse DFT with
phase unwrapping. All 8,853 source--receiver pairs satisfy the causality
criterion $E(t < t_{\text{arrival}}) / E_{\text{total}} < 10^{-4}$.

% -----------------------------------------------------------------------
\subsection{Forward Model Results}
\label{sec:forward_results}

Table~\ref{tab:forward_ablation} shows the forward model ablation.
The top block validates the transfer function formulation: a vanilla MLP
learning raw $\pscat$ achieves 48.00\% error---identical to the trivial
``no scatterer'' baseline ($\hat{p}=\pinc$, 47.95\%), confirming that
direct pressure learning fails entirely. Switching to the $T$ formulation
reduces error to 2.27\% (single model, 882 epochs) even \emph{without}
Fourier features, demonstrating that the transfer function---not the
encoding---is the key contribution. Fourier features accelerate
convergence but do not improve final accuracy given sufficient training.
The quad ensemble with calibration yields 4.47\% overall, with per-scene
errors from 0.93\% (Scene~1) to 18.62\% (Scene~13, deep shadow zone).
At inference, the neural forward model evaluates 50{,}000 pressure samples
in 130\,ms (GPU), achieving $2{,}000\times$ speedup over BEM
(260\,s per scene).

\begin{table}[t]
\centering
\caption{Forward model ablation. Top: target formulation comparison validates
that the transfer function (T) is essential. Bottom: ensemble and calibration.}
\label{tab:forward_ablation}
\begin{tabular}{llc}
\toprule
Configuration & Target & Error (\%) \\
\midrule
No scatterer ($\hat{p}=\pinc$) & --- & 47.95 \\
Vanilla MLP & $\pscat$ & 48.00 \\
No Fourier features$^*$ & $T$ & 2.27 \\
\midrule
Single model & $T$ & 11.54 \\
+ calibration & $T$ & 10.20 \\
Duo ensemble + calib & $T$ & 9.89 \\
Quad ensemble & $T$ & 4.57 \\
\textbf{Quad ens.\ + calib} & $T$ & \textbf{4.47} \\
\bottomrule
\multicolumn{3}{l}{\footnotesize $^*$Single model, 882 epochs (${\sim}3.2$\,h);
ensemble members train}\\
\multicolumn{3}{l}{\footnotesize \phantom{$^*$}${\sim}200$ epochs (${\sim}40$\,min) each with better efficiency.}
\end{tabular}
\end{table}

% -----------------------------------------------------------------------
\subsection{Inverse Reconstruction}
\label{sec:inverse_results}

Table~\ref{tab:inverse_ablation} shows the inverse model ablation.
Starting from SDF + Eikonal losses (IoU\,=\,0.69), adding boundary
oversampling ($+$0.15), cycle-consistency ($+$0.10), and multi-code
composition ($+$0.01) yields a final mean IoU of 0.9491.
Fourteen of 15 scenes achieve IoU\,$>$\,0.92 (Fig.~\ref{fig:sdf_gallery});
the sole exception is Scene~12 (two parallel plates, IoU\,=\,0.49),
where the smooth-minimum composition~\eqref{eq:smooth_min} struggles
with disjoint bodies.
Supplementary geometry metrics confirm reconstruction quality:
mean Chamfer distance 0.047\,m and mean Hausdorff distance 0.471\,m
across the 15 scenes (Hausdorff is dominated by wedge scenes S1--S4,
where open-mesh truncation creates edge artifacts at
$\mathrm{HD} > 0.8$\,m; closed-surface scenes achieve
$\mathrm{HD} < 0.035$\,m).

\begin{table}[t]
\centering
\caption{Inverse model ablation: cumulative loss components.}
\label{tab:inverse_ablation}
\begin{tabular}{lccc}
\toprule
Configuration & IoU & S12 & $r$ \\
\midrule
$\Lsdf + \Leik$ (200\,ep) & 0.689 & 0.135 & --- \\
$+$ bdy 3$\times$ (500\,ep) & 0.842 & 0.184 & --- \\
$+$ $\Lcyc$ (1000\,ep) & 0.939 & 0.410 & 0.909 \\
$+$ multi-code $K$=2 & \textbf{0.949} & 0.493 & 0.902 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{fig_4_sdf_gallery.pdf}
\caption{SDF reconstruction for four representative scenes.
Top: ground truth; bottom: predicted. The model achieves high fidelity for
single-body geometries (S1, S7, S10) but struggles with the disjoint
multi-body Scene~12 (IoU\,=\,0.49).}
\label{fig:sdf_gallery}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{fig_5_helmholtz_analysis.pdf}
\caption{Helmholtz PDE analysis.
(a)~Neural $\nabla^2 p$ (autodiff) vs.\ physical $\nabla^2 p = -k^2 p_{\rm BEM}$:
Pearson $r = 0.19$, explaining only 3.5\% of physical variance.
(b)~Second-derivative amplification $4\pi^2\sigma^2$ of random Fourier features:
at $\sigma = 30$, the amplification exceeds $35{,}000\times$.}
\label{fig:helmholtz}
\end{figure}

Cycle-consistency across all 15 scenes yields mean Pearson $r = 0.90$
(all $r > 0.83$). Notably, Scene~12 achieves $r = 0.92$ despite
IoU\,=\,0.49, showing cycle-consistency is necessary but
\emph{not sufficient} for geometry accuracy.

% -----------------------------------------------------------------------
\subsection{Robustness and Generalization}
\label{sec:robustness}

\textbf{Robustness.}
Adding complex Gaussian noise at 10--40\,dB SNR shows graceful
degradation: $r = 0.86$ at 10\,dB ($\Delta r = -0.04$).
Three-seed reproducibility confirms
IoU\,=\,$0.912 \pm 0.011$, $r = 0.907 \pm 0.001$ (all seeds pass gates).
Leave-one-out code optimization recovers 52\% mean IoU; wedge-like
shapes generalize (S1: 92\%, S14: 97\%) while novel shapes do not
(S5: 9\%, S10: 22\%).
Cross-frequency extrapolation (2--6\,kHz$\to$6--8\,kHz) fails at 43\% error:
the Fourier encoding treats $k$ as a positional input rather than a
continuous physical variable, so the model memorizes per-frequency patterns
instead of learning spectral structure.

% -----------------------------------------------------------------------
\subsection{Why Helmholtz PDE Loss Fails}
\label{sec:discussion}

Physics-informed approaches typically enforce
$\|\nabla^2 p + k^2 p\|^2 \to 0$ via automatic differentiation of a
neural field~\cite{raissi2019physics}. We investigate \emph{why} this
fails for our surrogate architecture and identify the root cause.

\textbf{Empirical measurement.}
We compute the neural Laplacian $\nabla^2_{\text{auto}} p$ via second-order
autodiff of $f_\theta$ w.r.t.\ receiver coordinates, and compare it against
the physical Laplacian $\nabla^2_{\text{phys}} p = -k^2 p$ (which holds
exactly for the BEM ground truth by the Helmholtz equation).
Over 10{,}000 evaluation points across 5 scenes
(Fig.~\ref{fig:helmholtz}a), the Pearson correlation is
$r = 0.19$---the neural Laplacian explains only 3.5\% of the physical
Laplacian's variance. The median Helmholtz residual
$|\nabla^2_{\text{auto}} p + k^2 p|$ is $\mathcal{O}(10^3)$,
confirming that the neural $\nabla^2$ is dominated by network curvature.
Enabling this loss collapsed IoU from 0.82 to 0.19 within 30 epochs.

\textbf{Cause: Fourier feature amplification.}
Wang et al.~\cite{wang2021eigenvector} showed that random Fourier features
modulate NTK convergence rates. We extend this to identify a practical
failure mode: the encoding $\gamma(\bm{v}) = \cos(2\pi \bm{B}\bm{v})$
with $B_{ij} \sim \mathcal{N}(0, \sigma^2)$ has second derivatives
$\partial^2 \gamma / \partial v_j^2 = -(2\pi B_j)^2 \gamma(\bm{v})$,
yielding an expected amplification
$\mathbb{E}[(2\pi B_j)^2] = 4\pi^2 \sigma^2$.
At $\sigma = 30$\,m$^{-1}$,
this gives $4\pi^2 \cdot 900 \approx 35{,}000\times$ amplification
(Fig.~\ref{fig:helmholtz}b). However, reducing $\sigma$ to~1 lowers
the residual to $\mathcal{O}(10^1)$ without eliminating it---a surrogate
trained on pressure values has no incentive to match second derivatives.
Fourier amplification is an aggravating factor, not the sole cause.

\textbf{Asymmetry with Eikonal.}
The Eikonal constraint $|\nabla s| = 1$ succeeds because it requires
only \emph{first}-order gradients of the SDF decoder's own output, where
network gradients align with the physical quantity.
Helmholtz requires \emph{second}-order gradients of a different network
(the frozen forward model), through a high-bandwidth Fourier encoding.
This first- vs.\ second-order asymmetry explains why geometry constraints
work while wave-equation constraints do not.

% ========================================================================
% 4. CONCLUSION
% ========================================================================
\section{Conclusion}
\label{sec:conclusion}

We presented a cycle-consistent neural framework for 2D acoustic
diffraction tomography. The transfer function formulation is the central
enabler: it reduces forward error from 48\% to 4.47\% while providing
$2{,}000\times$ speedup over BEM (0.13\,s vs.\ 260\,s per scene).
A single model without Fourier features reaches 2.27\%,
confirming the formulation---not the encoding---as the key factor.
The auto-decoder inverse model achieves
$0.91 \pm 0.01$ mean IoU, validated by cycle-consistency
($r = 0.907 \pm 0.001$) robust to 10\,dB noise.

Our Helmholtz PDE analysis reveals a fundamental obstacle for
physics-informed neural surrogates: random Fourier features amplify
second derivatives by $\sim$35{,}000$\times$ at $\sigma\!=\!30$, reducing
the neural-physical Laplacian correlation to $r = 0.19$; even at
$\sigma\!=\!1$, the residual remains $\mathcal{O}(10^1)$, confirming that
surrogate-learned curvature fundamentally diverges from the physical
Laplacian.
This has implications for the PINN literature on acoustic surrogate models.

Limitations include: 2D synthetic data only (15 scenes),
per-scene optimization, limited cross-frequency generalization,
and disjoint geometry difficulty (S12 IoU\,=\,0.49). Future work
targets 3D extension, frequency-continuous architectures, and
differentiable BEM solvers for physically grounded PDE enforcement.

% ========================================================================
% REFERENCES
% ========================================================================
\bibliographystyle{IEEEtran}
\bibliography{refs}

\end{document}
